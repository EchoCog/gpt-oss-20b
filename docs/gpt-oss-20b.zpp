// GPT-OSS 20B Z++ Specification (topology, routing, precision)

[ TOKEN, POSITION, EXPERT, BYTE, BLOCK ]

N_LAYERS == 24
VOCAB_SIZE == 201088
H_RESID == 2880
NUM_HEADS_Q == 64
NUM_HEADS_KV == 8
HEAD_DIM == 64
H_ATTN == NUM_HEADS_Q * HEAD_DIM
SLIDING_WINDOW == 128
MAX_POS == 131072
ROPE_THETA == 150000
ROPE_YARN_FACTOR == 32
ROUTER_TOP_K == 4
NUM_LOCAL_EXPERTS == 32
ATTN_DROPOUT == 0.0
RMS_EPS == 1e-5
ROUTER_AUX_COEF == 0.9
TIE_EMBEDDINGS == false
ATTN_BIAS == true

AttnKind ::= Sliding | Full
DType ::= BF16 | FP4 | UE8

SpecialTokenId == {
  START_OF_TEXT: 199998,
  END_OF_TEXT: 199999,
  RETURN: 200002,
  CONSTRAIN: 200003,
  CHANNEL: 200005,
  START: 200006,
  END: 200007,
  MESSAGE: 200008,
  END_OF_PROMPT: 200018
}

class RoPE
{
  const theta: ‚Ñï; factor: ‚Ñï; max_pos: ‚Ñï
  inv theta = ROPE_THETA ‚àß factor = ROPE_YARN_FACTOR ‚àß max_pos = MAX_POS

  rope_embed: POSITION √ó 1..HEAD_DIM ‚Üí ‚Ñù √ó ‚Ñù
  pre ‚àÄ p: POSITION ‚Ä¢ p ‚â§ max_pos
}

class AttentionLayout
{
  const num_layers: ‚Ñï; sliding_window: ‚Ñï
  inv num_layers = N_LAYERS ‚àß sliding_window = SLIDING_WINDOW

  attn_type: 0..(N_LAYERS‚àí1) ‚Üí AttnKind
  inv ‚àÄ i: 0..(N_LAYERS‚àí1) ‚Ä¢
        (i mod 2 = 0 ‚áí attn_type i = Sliding) ‚àß
        (i mod 2 = 1 ‚áí attn_type i = Full)
}

class ParameterDTypes
{
  is_attn_param, is_embed_param, is_router_param, is_mlp_blocks, is_mlp_scales: STRING ‚Üí ùîπ

  dtype_of: STRING ‚Üí DType
  inv ‚àÄ n: STRING ‚Ä¢
      (is_attn_param n ‚à® is_embed_param n ‚à® is_router_param n) ‚áí dtype_of n = BF16 ‚àß
      (is_mlp_blocks n ‚áí dtype_of n = FP4) ‚àß
      (is_mlp_scales n ‚áí dtype_of n = UE8)
}

class Gating
{
  const num_experts: ‚Ñï; top_k: ‚Ñï
  inv num_experts = NUM_LOCAL_EXPERTS ‚àß top_k = ROUTER_TOP_K ‚àß top_k < num_experts

  route: TOKEN ‚Üí ‚Ñô EXPERT
  score: TOKEN √ó EXPERT ‚Üí ‚Ñù

  inv ‚àÄ t: TOKEN ‚Ä¢ #(route t) = top_k ‚àß route t ‚äÜ EXPERT
  aux_loss_coef: ‚Ñù
  inv aux_loss_coef = ROUTER_AUX_COEF
}

class MoEExperts
{
  W1_blocks: EXPERT ‚Üí BLOCK
  W1_scales: EXPERT ‚Üí BYTE
  W2_blocks: EXPERT ‚Üí BLOCK
  W2_scales: EXPERT ‚Üí BYTE

  fwd_expert: EXPERT √ó ‚Ñù^H_RESID ‚Üí ‚Ñù^H_RESID
}

class AttentionCore
{
  q_proj: ‚Ñù^H_RESID ‚Üí ‚Ñù^H_ATTN
  k_proj: ‚Ñù^H_RESID ‚Üí ‚Ñù^(NUM_HEADS_KV * HEAD_DIM)
  v_proj: ‚Ñù^H_RESID ‚Üí ‚Ñù^(NUM_HEADS_KV * HEAD_DIM)
  o_proj: ‚Ñù^H_ATTN ‚Üí ‚Ñù^H_RESID

  rope: RoPE
  bias_enabled: ùîπ
  inv bias_enabled = ATTN_BIAS

  attention_step:
    AttnKind √ó POSITION √ó seq ‚Ñù^H_RESID ‚Üí ‚Ñù^H_RESID
  pre ‚àÄ pos ‚Ä¢ pos ‚â§ rope.max_pos
  post
    (Œæ? = Sliding ‚áí only tokens within SLIDING_WINDOW of pos contribute)
    ‚àß (Œæ? = Full ‚áí any previous token may contribute)
}

class TransformerLayer
{
  kind: AttnKind
  attn: AttentionCore
  router: Gating
  experts: MoEExperts

  in_rms_scale, post_attn_rms_scale: ‚Ñù^H_RESID
  inv ‚àÄ i: 1..H_RESID ‚Ä¢ finite(in_rms_scale[i]) ‚àß finite(post_attn_rms_scale[i])

  forward:
    POSITION √ó seq ‚Ñù^H_RESID ‚Üí ‚Ñù^H_RESID
}

class GPT_OSS_MoE
{
  const vocab_size: ‚Ñï; n_layers: ‚Ñï; h_resid: ‚Ñï
  inv vocab_size = VOCAB_SIZE ‚àß n_layers = N_LAYERS ‚àß h_resid = H_RESID

  layout: AttentionLayout
  params: ParameterDTypes

  embed: TOKEN ‚Üí ‚Ñù^H_RESID
  unembed: ‚Ñù^H_RESID ‚Üí ‚Ñù^VOCAB_SIZE
  inv TIE_EMBEDDINGS = false

  layers: 0..(N_LAYERS‚àí1) ‚Üí TransformerLayer
  inv ‚àÄ i ‚Ä¢ (layers i).kind = layout.attn_type i

  use_cache: ùîπ; attn_dropout: ‚Ñù; rms_eps: ‚Ñù
  inv use_cache = true ‚àß attn_dropout = ATTN_DROPOUT ‚àß rms_eps = RMS_EPS

  generate_step:
    POSITION √ó seq TOKEN ‚Üí TOKEN
  pre length input ‚â• 1 ‚àß current_pos ‚â§ MAX_POS
}